{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Question Answering (QA) is a field within natural language processing focused on designing systems that can answer questions. Among the more famous question answering systems is Watson, the IBM computer that competed (and won) on Jeopardy!. A question answering system of Watson’s accuracy requires enormous complexity and vast amounts of data, but in this problem, we’ll design a very simple question answering system based on inverse document frequency.\n",
    "\n",
    "Our question answering system will perform two tasks: document retrieval and passage retrieval. Our system will have access to a corpus of text documents. When presented with a query (a question in English asked by the user), document retrieval will first identify which document(s) are most relevant to the query. Once the top documents are found, the top document(s) will be subdivided into passages (in this case, sentences) so that the most relevant passage to the question can be determined.\n",
    "\n",
    "How do we find the most relevant documents and passages? To find the most relevant documents, we’ll use tf-idf to rank documents based both on term frequency for words in the query as well as inverse document frequency for words in the query. Once we’ve found the most relevant documents, there many possible metrics for scoring passages, but we’ll use a combination of inverse document frequency and a query term density measure (described in the Specification).\n",
    "\n",
    "More sophisticated question answering systems might employ other strategies (analyzing the type of question word used, looking for synonyms of query words, lemmatizing to handle different forms of the same word, etc.) but we’ll leave those sorts of improvements as exercises for you to work on if you’d like to after you’ve completed this project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sys\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import functools \n",
    "FILE_MATCHES = 1\n",
    "SENTENCE_MATCHES = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(directory):\n",
    "    \"\"\"\n",
    "    Given a directory name, return a dictionary mapping the filename of each\n",
    "    `.txt` file inside that directory to the file's contents as a string.\n",
    "    \"\"\"\n",
    "    res = dict()\n",
    "    filenames = os.listdir(directory)\n",
    "    for filename in filenames:\n",
    "        path = os.path.join(directory, filename)\n",
    "        sentence = \"\"\n",
    "        with open(path, \"r\", encoding='UTF-8') as file:\n",
    "            for f in file.readlines():\n",
    "                sentence += f.strip() + \" \"\n",
    "        res[filename] = sentence\n",
    "        \n",
    "    return res\n",
    "\n",
    "def tokenize(document):\n",
    "    \"\"\"\n",
    "    Given a document (represented as a string), return a list of all of the\n",
    "    words in that document, in order.\n",
    "\n",
    "    Process document by coverting all words to lowercase, and removing any\n",
    "    punctuation or English stopwords.\n",
    "    \"\"\"\n",
    "    tmp = nltk.word_tokenize(document)\n",
    "    words = [t.lower() for t in tmp]\n",
    "    res = []\n",
    "    for word in words:\n",
    "        if word not in string.punctuation and word not in nltk.corpus.stopwords.words(\"english\"):\n",
    "            res.append(word)\n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "def compute_idfs(documents):\n",
    "    \"\"\"\n",
    "    Given a dictionary of `documents` that maps names of documents to a list\n",
    "    of words, return a dictionary that maps words to their IDF values.\n",
    "\n",
    "    Any word that appears in at least one of the documents should be in the\n",
    "    resulting dictionary.\n",
    "    \"\"\"\n",
    "    res = dict()\n",
    "    n = len(documents)\n",
    "    tmp_documents = dict()\n",
    "    all_words = set()\n",
    "    for filename in documents:\n",
    "        tmp_documents[filename] = set(documents[filename])\n",
    "        all_words.update(tmp_documents[filename])\n",
    "\n",
    "    for word in all_words:\n",
    "        cnt = 0\n",
    "        for filename in tmp_documents:\n",
    "            if word in tmp_documents[filename]:\n",
    "                cnt += 1\n",
    "        res[word] = np.log(n / cnt)\n",
    "        \n",
    "    return res\n",
    "            \n",
    "def compute_tf(word, words):\n",
    "    cnt = 0\n",
    "    for w in words:\n",
    "        if w == word:\n",
    "            cnt += 1\n",
    "    \n",
    "    return cnt\n",
    "\n",
    "def top_files(query, files, idfs, n):\n",
    "    \"\"\"\n",
    "    Given a `query` (a set of words), `files` (a dictionary mapping names of\n",
    "    files to a list of their words), and `idfs` (a dictionary mapping words\n",
    "    to their IDF values), return a list of the filenames of the the `n` top\n",
    "    files that match the query, ranked according to tf-idf.\n",
    "    \"\"\"\n",
    "    tmp = []\n",
    "    for filename in files:\n",
    "        score = 0\n",
    "        for word in query:\n",
    "            if word in idfs:\n",
    "                score += compute_tf(word, files[filename]) * idfs[word]\n",
    "        tmp.append((filename, score))\n",
    "        \n",
    "    tmp.sort(key=lambda x: -x[1])\n",
    "    res = []\n",
    "    for i in range(n):\n",
    "        res.append(tmp[i][0])\n",
    "    \n",
    "    return res\n",
    "    \n",
    "def compute_cnt(word, sentence):\n",
    "    score = 0\n",
    "    for s in sentence:\n",
    "        if s == word:\n",
    "            score += 1\n",
    "    \n",
    "    return score / len(sentence)\n",
    "\n",
    "#From small to large\n",
    "def cmp(a, b):\n",
    "    if a[1] != b[1]:\n",
    "        return b[1] - a[1]\n",
    "    else:\n",
    "        return b[2] - a[2]\n",
    "\n",
    "def top_sentences(query, sentences, idfs, n):\n",
    "    \"\"\"\n",
    "    Given a `query` (a set of words), `sentences` (a dictionary mapping\n",
    "    sentences to a list of their words), and `idfs` (a dictionary mapping words\n",
    "    to their IDF values), return a list of the `n` top sentences that match\n",
    "    the query, ranked according to idf. If there are ties, preference should\n",
    "    be given to sentences that have a higher query term density.\n",
    "    \"\"\"\n",
    "    tmp = []\n",
    "    for sentence in sentences:\n",
    "        s1 = 0\n",
    "        s2 = 0\n",
    "        for word in query:\n",
    "            if word in sentences[sentence]:\n",
    "                s1 += idfs[word]\n",
    "            s2 += compute_cnt(word, sentences[sentence])\n",
    "        tmp.append((sentence, s1, s2))\n",
    "        \n",
    "    tmp = sorted(tmp, key=functools.cmp_to_key(cmp))\n",
    "    \n",
    "    res = []\n",
    "    for i in range(n):\n",
    "        res.append(tmp[i][0])\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run our algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: When was Python 3.0 released?\n",
      "Python 3.0 was released on 3 December 2008.\n"
     ]
    }
   ],
   "source": [
    "Dir=\"C:/Users/rzouga/Downloads/harvard/questions/questions/corpus\"\n",
    "# Calculate IDF values across files\n",
    "files = load_files(Dir)\n",
    "file_words = {\n",
    "    filename: tokenize(files[filename])\n",
    "    for filename in files\n",
    "}\n",
    "file_idfs = compute_idfs(file_words)\n",
    "\n",
    "# Prompt user for query\n",
    "query = set(tokenize(input(\"Query: \")))\n",
    "\n",
    "# Determine top file matches according to TF-IDF\n",
    "filenames = top_files(query, file_words, file_idfs, n=FILE_MATCHES)\n",
    "\n",
    "# Extract sentences from top files\n",
    "sentences = dict()\n",
    "for filename in filenames:\n",
    "    for passage in files[filename].split(\"\\n\"):\n",
    "        for sentence in nltk.sent_tokenize(passage):\n",
    "            tokens = tokenize(sentence)\n",
    "            if tokens:\n",
    "                sentences[sentence] = tokens\n",
    "\n",
    "# Compute IDF values across sentences\n",
    "idfs = compute_idfs(sentences)\n",
    "\n",
    "# Determine top sentence matches\n",
    "matches = top_sentences(query, sentences, idfs, n=SENTENCE_MATCHES)\n",
    "for match in matches:\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query: What are the types of supervised learning?\n",
    "Query: When was Python 3.0 released?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
