{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sys\n",
    "\n",
    "TERMINALS = \"\"\"\n",
    "Adj -> \"country\" | \"dreadful\" | \"enigmatical\" | \"little\" | \"moist\" | \"red\"\n",
    "Adv -> \"down\" | \"here\" | \"never\"\n",
    "Conj -> \"and\"\n",
    "Det -> \"a\" | \"an\" | \"his\" | \"my\" | \"the\"\n",
    "N -> \"armchair\" | \"companion\" | \"day\" | \"door\" | \"hand\" | \"he\" | \"himself\"\n",
    "N -> \"holmes\" | \"home\" | \"i\" | \"mess\" | \"paint\" | \"palm\" | \"pipe\" | \"she\"\n",
    "N -> \"smile\" | \"thursday\" | \"walk\" | \"we\" | \"word\"\n",
    "P -> \"at\" | \"before\" | \"in\" | \"of\" | \"on\" | \"to\" | \"until\"\n",
    "V -> \"arrived\" | \"came\" | \"chuckled\" | \"had\" | \"lit\" | \"said\" | \"sat\"\n",
    "V -> \"smiled\" | \"tell\" | \"were\"\n",
    "\"\"\"\n",
    "\n",
    "NONTERMINALS = \"\"\"\n",
    "S -> N V\n",
    "\"\"\"\n",
    "\n",
    "grammar = nltk.CFG.fromstring(NONTERMINALS + TERMINALS)\n",
    "parser = nltk.ChartParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge(word):\n",
    "    flag = False\n",
    "    for s in word:\n",
    "        if s.isalpha():\n",
    "            flag = True\n",
    "            break\n",
    "    \n",
    "    return flag\n",
    "\n",
    "def preprocess(sentence):\n",
    "    \"\"\"\n",
    "    Convert `sentence` to a list of its words.\n",
    "    Pre-process sentence by converting all characters to lowercase\n",
    "    and removing any word that does not contain at least one alphabetic\n",
    "    character.\n",
    "    \"\"\"\n",
    "    words = nltk.wordpunct_tokenize(sentence)\n",
    "    res = []\n",
    "    for word in words:\n",
    "        word_lower = word.lower()\n",
    "        if judge(word_lower):\n",
    "            res.append(word_lower)\n",
    "    \n",
    "    return res\n",
    "\n",
    "def np_chunk_helper(tree, List):\n",
    "    if tree == None:\n",
    "        return\n",
    "    #Calculate the number of NP\n",
    "    cnt = 0\n",
    "    tmp = []\n",
    "    for t in tree.subtrees():\n",
    "        if t.label() == \"NP\":\n",
    "            tmp.append(t)\n",
    "            cnt += 1\n",
    "    #Update if there is only 1 NP\n",
    "    if cnt == 1:\n",
    "        t = tmp[0]\n",
    "        #Prevent duplication\n",
    "        if t not in List:\n",
    "            List.append(t)\n",
    "        return\n",
    "    elif cnt > 1:\n",
    "        for t in tree.subtrees():\n",
    "            #Exclude yourself\n",
    "            if t != tree:\n",
    "                np_chunk_helper(t, List)\n",
    "    \n",
    "        return\n",
    "    \n",
    "    return\n",
    "\n",
    "def np_chunk(tree):\n",
    "    \"\"\"\n",
    "    Return a list of all noun phrase chunks in the sentence tree.\n",
    "    A noun phrase chunk is defined as any subtree of the sentence\n",
    "    whose label is \"NP\" that does not itself contain any other\n",
    "    noun phrases as subtrees.\n",
    "    \"\"\"\n",
    "    List = []\n",
    "    np_chunk_helper(tree, List)\n",
    "    \n",
    "    return List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Holmes sat\n",
      "        S     \n",
      "   _____|___   \n",
      "  N         V \n",
      "  |         |  \n",
      "holmes     sat\n",
      "\n",
      "Noun Phrase Chunks\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # If filename specified, read sentence from file\n",
    "    if len(sys.argv) == 2:\n",
    "        with open(sys.argv[1]) as f:\n",
    "            s = f.read()\n",
    "\n",
    "    # Otherwise, get sentence as input\n",
    "    else:\n",
    "        s = input(\"Sentence: \")\n",
    "\n",
    "    # Convert input into list of words\n",
    "    s = preprocess(s)\n",
    "\n",
    "    # Attempt to parse sentence\n",
    "    try:\n",
    "        trees = list(parser.parse(s))\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        return\n",
    "    if not trees:\n",
    "        print(\"Could not parse sentence.\")\n",
    "        return\n",
    "\n",
    "    # Print each tree with noun phrase chunks\n",
    "    for tree in trees:\n",
    "        tree.pretty_print()\n",
    "\n",
    "        print(\"Noun Phrase Chunks\")\n",
    "        for np in np_chunk(tree):\n",
    "            print(\" \".join(np.flatten()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
